---
title:          "Membership Inference Attacks Against Vision-Language Models"
date:           2025-01-02
selected:       true
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "USENIX Security 2025"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: aa7d500086c0ad534dbe6a705003d794573d925a  # use this to retrieve citation count
abstract: >-
  Training a machine learning model with data following a meaningful order, i.e., from easy to hard, has been proven to be effective in accelerating the training process and achieving better model performance. The key enabling technique is curriculum learning(CL), which has seen great success and has been deployed in areas like image and text classiffcation. Yet, how CL affects the privacy of machine learning is unclear. Given that CL changes the way a model memorizes the training data, its inffuence on data privacy needs to be thoroughly evaluated. To ffll this knowledge gap, we perform the first study and leverage membership inference attack (MIA) and attribute inference attack (AIA) as two vectors to quantify the privacy leakage caused by CL.
cover:          /assets/images/covers/joint_teaser.png
authors:
  - Joann Qiongna Chen
  - Xinlei He
  - Zheng Li
  - Yang Zhang
  - Zhou Li
links:
  Paper: https://par.nsf.gov/biblio/10637160
---
